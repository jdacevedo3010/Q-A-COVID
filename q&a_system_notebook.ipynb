{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11419891",
   "metadata": {},
   "source": [
    "# Q&A System fro COVID-19 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f97ef",
   "metadata": {},
   "source": [
    "Q&A Systems are Artificial Intelligences that have the task of answering user made questions with accurate and reliable information in a quick and efficient manner, given the high amount of academic papers being done about Covid-19 research, it is highly convenient to develop models that mine those articles and allows the users to have access to information relating Covid-19. The answered question can be about: certain person risk propension, different possible symptoms, average duration of the symptoms, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a3cfe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "from itertools import chain\n",
    "import jsonlines\n",
    "from src import QaModule, print_answers_in_file\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af33e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Settings\n",
    "settings_file = open(\"./config.json\")\n",
    "settings = json.load(settings_file)\n",
    "data_path = settings[\"data_path\"]\n",
    "models_path = settings[\"models_path\"]\n",
    "\n",
    "settings_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38812a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the path to get the paragraphs from the Doc Retriever module and path to save the answers\n",
    "ir_path = f\"{data_path}/ir.jsonl\"\n",
    "answers_path = f\"{data_path}/qa.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f015d92",
   "metadata": {},
   "source": [
    "# Writing of the answers retrieved for each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a595d09",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/QA/2021-05-24/ir.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0a9a926a79da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mir_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mir_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswers_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mans_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     data_for_qa = (\n\u001b[0;32m      4\u001b[0m         \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mir_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/QA/2021-05-24/ir.jsonl'"
     ]
    }
   ],
   "source": [
    "with open(ir_path) as ir_file, open(answers_path, \"w\") as ans_file:\n",
    "\n",
    "    data_for_qa = (\n",
    "        json.loads(data)\n",
    "        for data in ir_file\n",
    "    )\n",
    "    \n",
    "    #Declare the constructed QaModule, this is imported but can be checked in below cells\n",
    "    #Here we are using both pre-trained models: BioBERT and HLTC-MRQA, if the MRQA model wants to be taken off it can through the change of parameters\n",
    "    qa_model = QaModule([\"mrqa\", \"biobert\"], [f\"{models_path}/HLTC-MRQA/exported-tf-model-1.15.2\", f\"{models_path}/BioBERT/exported-tf-model-1.15.2\"], \\\n",
    "        f\"{models_path}/HLTC-MRQA/spiece.model\", f\"{models_path}/BioBERT/bert_config.json\", f\"{models_path}/BioBERT/vocab.txt\")\n",
    "\n",
    "    print(\"Get Answers...\")\n",
    "    answers = qa_model.getAnswers(data_for_qa)\n",
    "    \n",
    "    #Writes the text with the reranked retrieved answers\n",
    "    for ans in answers:\n",
    "        q = ans[\"question\"]\n",
    "        answers = list(ans[\"data\"][\"answer\"])\n",
    "        print(\"Q: \", q)\n",
    "        print(answers)\n",
    "        json.dump({\n",
    "            \"question\": q,\n",
    "            \"answers\": answers\n",
    "        }, ans_file)\n",
    "        ans_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92301033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declare the path for the qa jsonl\n",
    "qa_path =  './qa.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f10ae9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the incubation period of the virus?\n",
      " ['51 Infectious Disease Epidemiology  The incubation period is the time interval between the invasion by a microorganism and the first signs or symptoms of disease (onset of disease).', 'This assumption, together with the assumed incubation period of 1-14 days, implies a latent period of 0-9 days; or more specifically, the latent period is 0 if the incubation period ≤5 days and (incubation period duration -5 days) otherwise.', 'While we assume an infection can be infectious as early as 5 days before symptom onset or peak infectivity, the incubation period can be shorter than 5 days.', 'Epidemiology and Infection cambridge.org/hyg Original Paper  The incubation period of salmonellosis is usually 6-72 h [34] .', 'Current data for human A (H5N1) infection indicate an incubation period ranging from 2 to 8 days and possibly as long as 17 days. The incubation period for human avian influenza is longer than that of seasonal influenza, which generally is 2 or 3 days.', 'Viral Sepsis  Viruses that have short incubation periods of 2 to 5 days, usually affect the respiratory system.', 'Suppose individual is infected on day . We assume the incubation period =̃− has a known discrete distribution ( ) = ( = ), ≤ ≤ , where and are the minimum and maximum duration of the incubation period.', '  The incubation period is between 4 and 21 days, commonly about 10 days.', 'It can be between a few weeks and also takes up to several months or even a year (P. vivax or occasionally P. ovale).', '  • No answer (1%) 7. Which of the following best describes what an incubation period is?']\n",
      "What is the length of viral shedding after illness onset?\n",
      " ['• Viral shedding, which is often considered to determine the period of isolation as it could be a marker of infectivity, is considered to last for an average of 12-20 days; the longest duration of viral shedding for mild cases reported to date is 60 days.', 'The long-quarantined case of COVID-19 with prolonged viral shedding and intermittent fever for more than 70 days A 79-year old Japanese woman was diagnosed with coronavirus disease , caused by SARS coronavirus 2 (SARS-CoV-2), based on a positive reverse transcription-PCR (RT-PCR) test result.', 'The long-quarantined case of COVID-19 with prolonged viral shedding and intermittent fever for more than 70 days A 79-year old Japanese woman was diagnosed with coronavirus disease , caused by SARS coronavirus 2 (SARS-CoV-2), based on a positive reverse transcription-PCR (RT-PCR) test result.', 'Viral shedding peaks on the second day and in healthy adults is no longer detectable 6-10 days later. Pulmonary Infections 86  Influenza has an incubation period of 1-3 days, and viral shedding begins before the appearance of symptoms and within the first 24 h of inoculation.', 'Other Community Respiratory Viruses  One of the major questions is whether a positive RVP represents disease or is simply colonization or prolonged shedding from a prior unrelated infection.', ' Aim: To assess the effects of oral care on prolonged viral shedding in coronavirus disease 2019 (COVID-19) patients.', '  • No answer (1%) 7. Which of the following best describes what an incubation period is?', 'None declared. Did viral disease of humans wipe out the Neandertals?', '  Which viral group most commonly causes hand-foot-mouth disease?', '  How Does the Antiviral Defence Elicit Autoimmune Diseases?']\n"
     ]
    }
   ],
   "source": [
    "#Print the obtained answers for each question in COVID-QA\n",
    "count = 0\n",
    "with jsonlines.open(qa_path) as qa_file:\n",
    "    for line in qa_file.iter():\n",
    "        count = count + 1\n",
    "        if count < 3:\n",
    "            print(line['question'], line['answers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7dd19f",
   "metadata": {},
   "source": [
    "# Description of the QaModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dc071a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]\n",
    "\n",
    "class QaModule():\n",
    "    def __init__(self, model_name, model_path, spiece_model, bert_config, bert_vocab):\n",
    "        # init QA models\n",
    "        self.model_name = model_name\n",
    "        self.model_path = model_path\n",
    "        self.spiece_model = spiece_model\n",
    "        self.bert_config = bert_config\n",
    "        self.bert_vocab = bert_vocab\n",
    "        self.getPredictors()\n",
    "\n",
    "    def readIR(self, data):\n",
    "        '''\n",
    "        data: should be the iterable object with the queries, in our case data_for_qa\n",
    "        \n",
    "        Retrieves the documents that match according to the models, additionally if titles and identifiers\n",
    "        are given, keeps those values as well for further book keeping.\n",
    "        '''\n",
    "        synthetic = []\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        for data_item in data:\n",
    "            question = data_item[\"question\"]\n",
    "            answer = data_item[\"data\"][\"answer\"]\n",
    "            contexts = data_item[\"data\"][\"context\"]\n",
    "            dois = data_item[\"data\"][\"doi\"]\n",
    "            titles = data_item[\"data\"][\"titles\"]\n",
    "\n",
    "            for (context, doi, title) in zip(contexts, dois, titles):\n",
    "                data_sample = {\n",
    "                    \"context\": context,\n",
    "                    \"qas\": []\n",
    "                }\n",
    "\n",
    "                qas_item = {\n",
    "                    \"id\": idx,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer,\n",
    "                    \"doi\": doi,\n",
    "                    \"title\": title,\n",
    "                }\n",
    "\n",
    "                data_sample[\"qas\"].append(qas_item)\n",
    "                synthetic.append(data_sample)\n",
    "\n",
    "                idx += 1\n",
    "        return synthetic\n",
    "\n",
    "    def mrqaPredictor(self, data):\n",
    "        '''\n",
    "        Predicts the match between a snippet and a question according to HLTC-MRQA, this is MRQA's confidence value\n",
    "        '''\n",
    "        return mrqa_predictor(self.mrqaFLAGS, self.mrqa_predict_fn, data)\n",
    "    \n",
    "    def biobertPredictor(self, data):\n",
    "        '''\n",
    "        Predicts the match between a snippet and a question according to BioBERT, this is BioBERT's confidence value\n",
    "        '''\n",
    "        return biobert_predictor(self.bioFLAGS, self.bio_predict_fn, data)\n",
    "\n",
    "    def getPredictors(self):\n",
    "        '''\n",
    "        Gets the predictions according to the list of parameters given\n",
    "        '''\n",
    "        if \"mrqa\" in self.model_name:\n",
    "            self.mrqa_predict_fn = self.getPredictor(\"mrqa\")\n",
    "        if \"biobert\" in self.model_name:\n",
    "            self.bio_predict_fn = self.getPredictor(\"biobert\")\n",
    "\n",
    "    def getPredictor(self, model_name):\n",
    "        '''\n",
    "        Gets the pretrained models, it's implemented ONLY FOR MRQA AND BioBERT\n",
    "        '''\n",
    "        modelpath = self.getModelPath(model_name)\n",
    "        if model_name == 'mrqa':\n",
    "            d = {\n",
    "                \"uncased\": False,\n",
    "                \"start_n_top\": 5,\n",
    "                \"end_n_top\": 5,\n",
    "                \"use_tpu\": False,\n",
    "                \"train_batch_size\": 1,\n",
    "                \"predict_batch_size\": 1,\n",
    "                \"shuffle_buffer\": 2048,\n",
    "                \"spiece_model_file\": self.spiece_model,\n",
    "                \"max_seq_length\": 512,\n",
    "                \"doc_stride\": 128,\n",
    "                \"max_query_length\": 64,\n",
    "                \"n_best_size\": 5,\n",
    "                \"max_answer_length\": 64,\n",
    "            }\n",
    "            self.mrqaFLAGS = namedtuple(\"FLAGS\", d.keys())(*d.values())\n",
    "            return tf.contrib.predictor.from_saved_model(modelpath)\n",
    "        elif model_name == 'biobert':\n",
    "            d = {\n",
    "                \"version_2_with_negative\": False,\n",
    "                \"null_score_diff_threshold\": 0.0,\n",
    "                \"verbose_logging\": False,\n",
    "                \"init_checkpoint\": None,\n",
    "                \"do_lower_case\": False,\n",
    "                \"bert_config_file\": self.bert_config,\n",
    "                \"vocab_file\": self.bert_vocab,\n",
    "                \"train_batch_size\": 1,\n",
    "                \"predict_batch_size\": 1,\n",
    "                \"max_seq_length\": 384,\n",
    "                \"doc_stride\": 128,\n",
    "                \"max_query_length\": 64,\n",
    "                \"n_best_size\": 5,\n",
    "                \"max_answer_length\": 30,\n",
    "            }\n",
    "            self.bioFLAGS = namedtuple(\"FLAGS\", d.keys())(*d.values())\n",
    "            return tf.contrib.predictor.from_saved_model(modelpath)\n",
    "        else:\n",
    "            raise ValueError(\"invalid model name\")\n",
    "    \n",
    "    def getModelPath(self, model_name):\n",
    "        '''\n",
    "        Gets the model path\n",
    "        '''\n",
    "        index = self.model_name.index(model_name)\n",
    "        return self.model_path[index]\n",
    "\n",
    "    def getAnswers(self, data):\n",
    "        \"\"\"\n",
    "        Gets the answers for a given list of queries, this should be where data_for_qa object is passed\n",
    "        \n",
    "        Output:\n",
    "            List [{\n",
    "                \"question\": \"xxxx\",\n",
    "                \"data\": \n",
    "                    {\n",
    "                        \"answer\": [\"answer1\", \"answer2\", ...],\n",
    "                        \"confidence\": [1,2, ...],\n",
    "                        \"context\": [\"paragraph1\", \"paragraph2\", ...],\n",
    "                    }\n",
    "            }]\n",
    "        \"\"\"\n",
    "        answers = []\n",
    "        qas = self.readIR(data)\n",
    "        for qa in qas:\n",
    "            question = qa[\"qas\"][0][\"question\"]\n",
    "            #Checks if answer is not nothing and if the str is not the same as the question\n",
    "            if len(answers)==0 or answers[-1][\"question\"]!=question:\n",
    "                if len(answers) > 0:\n",
    "                    scores = answers[-1][\"data\"][\"confidence\"]\n",
    "                    #Computes the softmax between the confidence scores of each model\n",
    "                    answers[-1][\"data\"][\"confidence\"] = self._compute_softmax(scores)\n",
    "\n",
    "                answer_sample = {}\n",
    "                answer_sample[\"question\"] = question\n",
    "                answer_sample[\"data\"] = {\n",
    "                    \"answer\": [],\n",
    "                    \"context\": [],\n",
    "                    \"title\": [],\n",
    "                    \"doi\": [],\n",
    "                    \"confidence\": [],\n",
    "                    \"raw\": [],\n",
    "                }\n",
    "                #Appends answer\n",
    "                answers.append(answer_sample)\n",
    "\n",
    "            context = qa[\"context\"]\n",
    "            doi = qa[\"qas\"][0][\"doi\"]\n",
    "            title = qa[\"qas\"][0][\"title\"] \n",
    "\n",
    "            answers[-1][\"data\"][\"context\"].append(context)\n",
    "            answers[-1][\"data\"][\"doi\"].append(doi)\n",
    "            answers[-1][\"data\"][\"title\"].append(title)\n",
    "\n",
    "            sents = sent_tokenize(context)\n",
    "            spans = self.convert_idx(context, sents)\n",
    "            \n",
    "            raw_score_mrqa = 0\n",
    "            raw_score_bio = 0\n",
    "            \n",
    "            if \"mrqa\" in self.model_name:\n",
    "                raw_mrqa = self.mrqaPredictor([qa])\n",
    "                # get sentence from MRQA\n",
    "                raw = raw_mrqa[qa[\"qas\"][0][\"id\"]]   \n",
    "                raw_answer_mrqa = raw[0]\n",
    "                raw_score_mrqa = raw[1]\n",
    "\n",
    "                # question answering one by one\n",
    "                answer_start = context.find(raw_answer_mrqa, 0)\n",
    "                answer_end = answer_start + len(raw_answer_mrqa)\n",
    "                answer_span = []\n",
    "                for idx, span in enumerate(spans):\n",
    "                    if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                        answer_span.append(idx)\n",
    "\n",
    "                y1, y2 = answer_span[0], answer_span[-1]\n",
    "                if not y1 == y2:\n",
    "                    # context tokens in index y1 and y2 should be merged together\n",
    "                    # print(\"Merge knowledge sentence\")\n",
    "                    answer_sent_mrqa = \" \".join(sents[y1:y2+1])\n",
    "                else:\n",
    "                    answer_sent_mrqa = sents[y1]\n",
    "                assert raw_answer_mrqa in answer_sent_mrqa\n",
    "            else:\n",
    "                answer_sent_mrqa = \"\"\n",
    "            \n",
    "            \n",
    "            if \"biobert\" in self.model_name:\n",
    "                raw_bio = self.biobertPredictor([qa])\n",
    "                # get sentence from BioBERT\n",
    "                raw = raw_bio[qa[\"qas\"][0][\"id\"]]\n",
    "                raw_answer_bio = raw[0]\n",
    "                raw_score_bio = raw[1] \n",
    "\n",
    "                if raw_answer_bio == \"empty\" or \"\":\n",
    "                    answer_sent_bio = \"\"\n",
    "                    raw_score_bio = 0\n",
    "                else:\n",
    "                    # question answering one by one\n",
    "                    answer_start = context.find(raw_answer_bio, 0)\n",
    "                    answer_end = answer_start + len(raw_answer_bio)\n",
    "                    answer_span = []\n",
    "                    for idx, span in enumerate(spans):\n",
    "                        if not (answer_end <= span[0] or answer_start >= span[1]):\n",
    "                            answer_span.append(idx)\n",
    "\n",
    "                    y1, y2 = answer_span[0], answer_span[-1]\n",
    "                    if not y1 == y2:\n",
    "                        # context tokens in index y1 and y2 should be merged together\n",
    "                        # print(\"Merge knowledge sentence\")\n",
    "                        answer_sent_bio = \" \".join(sents[y1:y2+1])\n",
    "                    else:\n",
    "                        answer_sent_bio = sents[y1]\n",
    "                    \n",
    "                    # if raw not in answer_sent_bio:\n",
    "                    #     print(\"RAW\", raw)\n",
    "                    #     print(\"BIO\", answer_sent_bio)\n",
    "                    # assert raw_answer_bio in answer_sent_bio\n",
    "            else:\n",
    "                answer_sent_bio = \"\"\n",
    "\n",
    "            if answer_sent_mrqa == answer_sent_bio or answer_sent_mrqa in answer_sent_bio:\n",
    "                # print(\"SAME OR QA < BIO\")\n",
    "                answer_sent = answer_sent_bio\n",
    "                if raw_score_mrqa < 0 and raw_score_bio < 0:\n",
    "                    if abs(raw_score_mrqa) < abs(raw_score_bio):\n",
    "                        score = abs(raw_score_mrqa) * 0.5 + raw_score_bio\n",
    "                    else:\n",
    "                        score = raw_score_mrqa + abs(raw_score_bio) * 0.5\n",
    "                else:\n",
    "                    score = raw_score_mrqa + raw_score_bio\n",
    "            elif answer_sent_bio in answer_sent_mrqa:\n",
    "                # print(\"BIO < QA\")\n",
    "                answer_sent = answer_sent_mrqa\n",
    "                if raw_score_mrqa < 0 and raw_score_bio < 0:\n",
    "                    if abs(raw_score_mrqa) < abs(raw_score_bio):\n",
    "                        score = abs(raw_score_mrqa) * 0.5 + raw_score_bio\n",
    "                    else:\n",
    "                        score = raw_score_mrqa + abs(raw_score_bio) * 0.5\n",
    "                else:\n",
    "                    score = raw_score_mrqa + raw_score_bio\n",
    "            else:\n",
    "                # print(\"DIFFERENT ANSWERS\")\n",
    "                answer_sent= \" \".join([answer_sent_mrqa, answer_sent_bio])\n",
    "                score = 0.5 * raw_score_mrqa + 0.5 * raw_score_bio\n",
    "            \n",
    "            if raw_answer_mrqa == raw_answer_bio or raw_answer_mrqa in raw_answer_bio:\n",
    "                # print(\"SAME OR QA < BIO\")\n",
    "                answer = [raw_answer_bio]\n",
    "            elif raw_answer_bio in raw_answer_mrqa:\n",
    "                # print(\"BIO < QA\")\n",
    "                answer = [answer_sent_mrqa]\n",
    "            else:\n",
    "                # print(\"DIFFERENT ANSWERS\")\n",
    "                answer = [raw_answer_mrqa, raw_answer_bio]\n",
    "            \n",
    "            answers[-1][\"data\"][\"answer\"].append(answer_sent)\n",
    "            answers[-1][\"data\"][\"raw\"].append(answer)\n",
    "            answers[-1][\"data\"][\"confidence\"].append(score)\n",
    "        \n",
    "        # rerank the answers\n",
    "        score_qa = get_rank_score(answers)\n",
    "        return score_qa\n",
    "    \n",
    "    def _compute_softmax(self, scores):\n",
    "        \"\"\"Compute softmax probability over scores.\"\"\"\n",
    "        if not scores:\n",
    "            return []\n",
    "\n",
    "        max_score = None\n",
    "        for score in scores:\n",
    "            if max_score is None or score > max_score:\n",
    "                max_score = score\n",
    "\n",
    "        exp_scores = []\n",
    "        total_sum = 0.0\n",
    "        for score in scores:\n",
    "            x = math.exp(score - max_score)\n",
    "            exp_scores.append(x)\n",
    "            total_sum += x\n",
    "\n",
    "        probs = []\n",
    "        for score in exp_scores:\n",
    "            probs.append(score / total_sum)\n",
    "        return probs\n",
    "    \n",
    "    def convert_idx(self, text, tokens):\n",
    "        current = 0\n",
    "        spans = []\n",
    "        for token in tokens:\n",
    "            current = text.find(token, current)\n",
    "            if current < 0:\n",
    "                print(\"Token {} cannot be found\".format(token))\n",
    "                raise Exception()\n",
    "            spans.append((current, current + len(token)))\n",
    "            current += len(token)\n",
    "        return spans\n",
    "\n",
    "def print_answers_in_file(answers, filepath=\"./answers.txt\"):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            List [{\n",
    "                \"question\": \"xxxx\",\n",
    "                \"data\": \n",
    "                    {\n",
    "                        \"answer\": [\"answer1\", \"answer2\", ...],\n",
    "                        \"confidence\": [1,2, ...],\n",
    "                        \"context\": [\"paragraph1\", \"paragraph2\", ...],\n",
    "                    }\n",
    "            }]\n",
    "        \"\"\"\n",
    "    with open(filepath, \"w\") as f:\n",
    "        print(\"WRITE ANSWERS IN FILES ...\")\n",
    "        for item in answers:\n",
    "            question = item[\"question\"]\n",
    "            cas = item[\"data\"]\n",
    "            for (answer, context) in zip(cas[\"answer\"], cas[\"context\"]):\n",
    "                f.write(\"-\"*80+\"\\n\")\n",
    "                f.write(\"context: \"+context+\"\\n\")\n",
    "                f.write(\"-\"*80+\"\\n\")\n",
    "                f.write(\"question: \"+question+\"\\n\")\n",
    "                f.write(\"-\"*80+\"\\n\")\n",
    "                f.write(\"answer: \"+answer+\"\\n\")\n",
    "            f.write(\"=\"*80+\"\\n\")\n",
    "\n",
    "def get_rank_score(qa_output):\n",
    "    '''\n",
    "    Gets the rerank score according to the paper's confidence and keyword based score, it rearranges the returned answers\n",
    "    for a query according to this new score.\n",
    "    '''\n",
    "    for item in qa_output:\n",
    "        query = item[\"question\"]\n",
    "        context = item['data']['context']\n",
    "        item['data']['matching_score'] = []\n",
    "        item['data']['rerank_score'] = []\n",
    "        # make new query with only n. and adj.\n",
    "        tokens = word_tokenize(query.lower())\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        tagged = pos_tag(tokens)\n",
    "        query_token = [tag[0] for tag in tagged if 'NN' in tag[1] or 'JJ' in tag[1] or 'VB' in tag[1]]\n",
    "\n",
    "        for i in range(len(context)):\n",
    "            text = context[i].lower()\n",
    "            count = 0\n",
    "            text_words = word_tokenize(text)\n",
    "            for word in text_words:\n",
    "                if word in query_token:\n",
    "                    count += 1\n",
    "            \n",
    "            # matching_score = count/len(text_words)*10 if len(text_words)>50 else count/len(text_words)   # short sentence penalty\n",
    "            matching_score = count / (1 + math.exp(-len(text_words)+50)) / 5\n",
    "            item['data']['matching_score'].append(matching_score)\n",
    "            item['data']['rerank_score'].append(matching_score + item['data']['confidence'][i])\n",
    "        \n",
    "        # sort QA results\n",
    "        c = list(zip(item['data']['rerank_score'], item['data']['context'], item['data']['answer'], item['data']['confidence'], item['data']['doi'], item['data']['title'], item['data']['matching_score'], item['data']['raw']))\n",
    "        c.sort(reverse = True)\n",
    "        item['data']['rerank_score'], item['data']['context'], item['data']['answer'], item['data']['confidence'], item['data']['doi'], item['data']['title'], item['data']['matching_score'], item['data']['raw'] = zip(*c)\n",
    "    return qa_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6c931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
